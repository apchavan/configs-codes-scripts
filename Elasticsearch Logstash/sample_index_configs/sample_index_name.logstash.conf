# Location for this config file can be at : '/etc/logstash/conf.d/' or anything suitable where logstash command can access and read it.

input {
    file {
        # Path to load input files from.
        path => "/PATH/TO/DATA_DIRECTORY/*.csv"
        start_position => "beginning"

        # File that keeps track of the current position of monitored log files
        sincedb_path => "/PATH/TO/sample_index_name_sincedb_file"  # Or can be "/dev/null"
        file_completed_action => "log"
        file_completed_log_path => "/dev/null"

        # Stop logstash process after reading all files from path
        exit_after_read => true
        mode => "read"
    }
}
filter {
    csv {
        # Columns to read from CSV file from input source
        columns => [
            "DATA_COLUMN_1",
            "DATA_COLUMN_2",
            "DATA_COLUMN_3",
            "DATA_COLUMN_4",
            "DATA_COLUMN_5",
            "DATE_COLUMN_6",
            "DATA_COLUMN_7",
            "DATE_COLUMN_8",
            "DATA_COLUMN_9",
            "DATA_COLUMN_10",
            "DATA_COLUMN_11"
        ]
        separator => ","
        remove_field => [
            "message"
        ]
        # Add extra 10 columns with `null` strings as default values.
        # The Logstash configuration language has no notion of null.
        # We'll store as an empty string.
        add_field => {
          "EXTRA_COLUMN_1" => ""
          "EXTRA_COLUMN_2" => ""
          "EXTRA_COLUMN_3" => ""
          "EXTRA_COLUMN_4" => ""
          "EXTRA_COLUMN_5" => ""
          "EXTRA_COLUMN_6" => ""
          "EXTRA_COLUMN_7" => ""
          "EXTRA_COLUMN_8" => ""
          "EXTRA_COLUMN_9" => ""
          "EXTRA_COLUMN_10" => ""
        }
    }
    # Cast data-type of `DATE_COLUMN_6` and `DATE_COLUMN_8` columns to date
    date {
        match => ["DATE_COLUMN_6", "yyyy-MM-dd"]
    }
    date {
        match => ["DATE_COLUMN_8", "yyyy-MM-dd"]
    }

    mutate {
        # Strip/trim white-spaces before & after
        strip => [
            "DATA_COLUMN_1",
            "DATA_COLUMN_2",
            "DATA_COLUMN_3",
            "DATA_COLUMN_4",
            "DATA_COLUMN_5"
        ]
        # Set column value datatypes
        convert => {
            "DATA_COLUMN_7" => "boolean"
            "DATA_COLUMN_9" => "boolean"
            "DATA_COLUMN_10" => "float"
            "DATA_COLUMN_11" => "float"
        }
    }
}
output {
    elasticsearch {
        hosts => ["http://localhost:9200"]
        user => "elastic"
        password => "PASSWORD_FOR_ELASTIC_USER"
        index => "sample_index_name"
        document_id => "%{DATA_COLUMN_1}_%{DATA_COLUMN_7}_%{DATA_COLUMN_9}"
        #ssl => true
        #cacert => "/usr/share/logstash/http_ca.crt"
    }
    #stdout { codec => rubydebug }
}

